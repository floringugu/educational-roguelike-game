# ğŸ¤– Modelos de IA Disponibles - GuÃ­a de SelecciÃ³n

## ğŸ“Š ComparaciÃ³n de Modelos

| Modelo | Disponibilidad | Velocidad | Calidad | Compatibilidad | Recomendado |
|--------|----------------|-----------|---------|----------------|-------------|
| **microsoft/Phi-3-mini-4k-instruct** | âœ… Siempre | âš¡âš¡âš¡âš¡âš¡ | â­â­â­â­â­ | âœ… text-gen | âœ… **SÃ** |
| **mistralai/Mistral-7B-Instruct-v0.2** | âœ… Siempre | âš¡âš¡âš¡âš¡ | â­â­â­â­â­ | âœ… text-gen | âœ… SÃ |
| **HuggingFaceH4/zephyr-7b-beta** | âœ… Siempre | âš¡âš¡âš¡âš¡ | â­â­â­â­â­ | âš ï¸ chat only | âš ï¸ Avanzado |
| **mistralai/Mixtral-8x7B-Instruct-v0.1** | âš ï¸ Variable | âš¡âš¡âš¡ | â­â­â­â­â­ | âœ… text-gen | âš ï¸ A veces |

---

## ğŸ¯ Modelo Recomendado: Phi-3-mini-4k-instruct

### âœ… Ventajas

- **Siempre disponible**: Funciona 24/7 en la API serverless de Hugging Face
- **El mÃ¡s rÃ¡pido**: Genera preguntas en 2-3 segundos
- **Excelente calidad**: Creado por Microsoft, optimizado para instrucciones
- **Gratis**: 100% gratuito con tu API key
- **MÃ¡xima compatibilidad**: Funciona con la API estÃ¡ndar de text-generation
- **Confiable**: No falla por "modelo no disponible" o "API incorrecta"

### ğŸ“ ConfiguraciÃ³n

Ya estÃ¡ configurado por defecto en tu `.env`:

```bash
HUGGINGFACE_MODEL=microsoft/Phi-3-mini-4k-instruct
```

---

## ğŸ”„ Modelos Alternativos

### 1. Mistral-7B-Instruct (Excelente Calidad)

Si prefieres el estilo de Mistral:

```bash
HUGGINGFACE_MODEL=mistralai/Mistral-7B-Instruct-v0.2
```

**CaracterÃ­sticas:**
- â­ Excelente calidad para preguntas educativas
- âœ… Siempre disponible
- ğŸ¯ Muy buena calidad de respuestas
- ğŸ”§ De los creadores de Mixtral (versiÃ³n mÃ¡s ligera)
- âš¡ RÃ¡pido (ligeramente mÃ¡s lento que Phi-3)

---

### 2. Zephyr-7b-beta (Avanzado)

âš ï¸ **Requiere API de chat (el cÃ³digo lo maneja automÃ¡ticamente)**

```bash
HUGGINGFACE_MODEL=HuggingFaceH4/zephyr-7b-beta
```

**CaracterÃ­sticas:**
- â­ Excelente calidad
- âœ… Siempre disponible
- ğŸ”§ Usa chat API en lugar de text-generation
- âš¡ RÃ¡pido
- âš ï¸ El cÃ³digo hace fallback automÃ¡tico si falla

---

### 3. Mixtral-8x7B (Mayor Calidad, pero...)

âš ï¸ **Solo usar si funciona en tu cuenta**

```bash
HUGGINGFACE_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1
```

**CaracterÃ­sticas:**
- â­ MÃ¡xima calidad
- âš ï¸ **NO siempre disponible** en API serverless gratuita
- ğŸŒ MÃ¡s lento (modelo muy grande: 47B parÃ¡metros)
- âŒ Puede dar error "modelo no disponible"

**Por quÃ© no estÃ¡ siempre disponible:**
- Es un modelo muy grande (pesa ~90GB)
- Hugging Face lo ejecuta en GPUs especiales
- Puede estar "durmiendo" si no se usa frecuentemente
- Tarda en "despertar" (loading state)

---

## ğŸ”§ Â¿CÃ³mo Cambiar de Modelo?

1. Abre el archivo `.env` en la raÃ­z del proyecto
2. Encuentra la lÃ­nea `HUGGINGFACE_MODEL=...`
3. ReemplÃ¡zala con el modelo que quieras
4. Guarda el archivo
5. Reinicia la aplicaciÃ³n (`python app.py`)

---

## ğŸ’¡ Recomendaciones

### Para uso general (RECOMENDADO):
```bash
HUGGINGFACE_MODEL=microsoft/Phi-3-mini-4k-instruct
```

### Si prefieres el estilo Mistral:
```bash
HUGGINGFACE_MODEL=mistralai/Mistral-7B-Instruct-v0.2
```

### Si eres usuario avanzado:
```bash
HUGGINGFACE_MODEL=HuggingFaceH4/zephyr-7b-beta
```

### Si Mixtral funciona para ti:
```bash
HUGGINGFACE_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1
```

---

## ğŸ› Troubleshooting

### Error: "Model not currently available"

**Causa:** El modelo estÃ¡ en estado "loading" o no disponible en API serverless.

**SoluciÃ³n:**
1. Cambia a Zephyr-7b-beta o Phi-3-mini
2. Estos modelos siempre estÃ¡n disponibles

### Error: "Model is loading"

**Causa:** El modelo grande estÃ¡ "despertando".

**SoluciÃ³n:**
- Espera 30-60 segundos y vuelve a intentar
- O cambia a un modelo mÃ¡s ligero

### Preguntas de baja calidad

**SoluciÃ³n:**
1. Prueba con Mistral-7B o Zephyr
2. Ambos generan excelentes preguntas educativas

---

## ğŸ“Š Rendimiento Comparativo

### Tiempo de generaciÃ³n (10 preguntas):

| Modelo | Tiempo Promedio |
|--------|-----------------|
| Phi-3-mini | ~2-3 segundos âš¡ |
| Mistral-7B | ~4-6 segundos |
| Zephyr-7b | ~4-5 segundos |
| Mixtral-8x7B | ~15 segundos (si estÃ¡ disponible) |

### Calidad de preguntas:

Todos los modelos recomendados generan preguntas de alta calidad para propÃ³sitos educativos. Las diferencias son mÃ­nimas en la prÃ¡ctica.

---

## âœ… ConclusiÃ³n

**Usa Phi-3-mini-4k-instruct** (configuraciÃ³n actual) - Es la mejor opciÃ³n:
- âœ… MÃ¡xima velocidad (el mÃ¡s rÃ¡pido)
- âœ… Disponibilidad garantizada 24/7
- âœ… Calidad excelente para preguntas educativas
- âœ… 100% gratis
- âœ… MÃ¡xima compatibilidad (text-generation API)
- âœ… Creado por Microsoft, muy confiable

**Solo cambia si:**
- Prefieres el estilo Mistral â†’ Mistral-7B-Instruct-v0.2
- Eres usuario avanzado â†’ Zephyr-7b-beta (usa chat API)
- Mixtral funciona para ti â†’ Mixtral-8x7B-Instruct-v0.1

---

**Fecha:** 2025-11-18
**VersiÃ³n:** 1.0
